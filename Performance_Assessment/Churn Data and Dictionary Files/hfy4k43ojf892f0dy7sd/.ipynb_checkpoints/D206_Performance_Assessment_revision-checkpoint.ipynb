{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Performance Assessment | D206 Data Cleaning<b>\n",
    "&emsp;Ryan L. Buchanan\n",
    "<br>&emsp;Student ID:  001826691\n",
    "<br>&emsp;Masters Data Analytics (12/01/2020)\n",
    "<br>&emsp;Program Mentor:  Dan Estes\n",
    "<br>&emsp;(385) 432-9281 (MST)\n",
    "<br>&emsp;rbuch49@wgu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part  I: Research Question</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>A. Question or Decision</b>:</span>\n",
    "Can we determine which individual customers are at high risk of churn?  And, can we determine which features are most significant to churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>B. Required Variables</b>:</span>\n",
    "The data set is 10,000 customer records of a popular telecommunications company. The dependent variable (target) in question is whether or not each customer has continued or discontinued service within the last month.  This column is titled \"Churn.\"  \n",
    "Independent variables or predictors that may lead to identifying a relationship with the dependent variable of \"Churn\" within the dataset include: \n",
    "1. Services that each customer signed up for (for example, multiple phone lines, technical support add-ons or streaming media) \n",
    "2. Customer account information (customers' tenure with the company, payment methods, bandwidth usage, etc.)\n",
    "3. Customer demographics (gender, marital status, income, etc.).  \n",
    "4. Finally, there are eight independent variables that represent responses customer-perceived importance of company services and features.  \n",
    "\n",
    "The data is both numerical (as in the yearly GB bandwidth usage; customer annual income) and categorical (a \"Yes\" or \"No\" for Churn; customer job)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part II: Data-Cleaning Plan</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C1. Plan to Find Anomalies</b>:</span>\n",
    "My approach will include:\n",
    "<br>&ensp; 1. Back up my data and the process I am following as a copy to my machine and, since this is a manageable dataset, to GitHub using command line and gitbash.\n",
    "<br>&ensp; 2. Read the data set into Python using Pandas' read_csv command.\n",
    "<br>&ensp; 3. Evaluate the data struture to better understand input data.\n",
    "<br>&ensp; 4. Naming the dataset as a the variable \"churn_df\" and subsequent useful slices of the dataframe as \"df\".\n",
    "<br>&ensp; 5. Examine potential misspellings, awkward variable naming & missing data.\n",
    "<br>&ensp; 6. Find outliers that may create or hide statistical significance using histograms.\n",
    "<br>&ensp; 7. Imputing records missing data with meaningful measures of central tendency (mean, median or mode) or simply remove outliers that are several standard deviations above the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C2. Justification of Approach</b>:</span>\n",
    "Though the data seems to be inexplicably missing quite a bit of data (such as the many NAs in customer tenure with the company) from apparently random columns, this approach seems like a good first approach in order to put the data in better working order without needing to involve methods of initial data collection or querying the data-gatherers on reasons for missing information. Also, this the first dataset that I've clean, so I followed the procedures practice in the performance lab as well as tips from StackOverflow and other tutorial resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C3. Justification of Tools</b>:</span>\n",
    "I will use the Python programming language as I have a bit of a background in Python having studied machine learning independently over the last year before beginning this masters program and its ability to perform many things right \"out of the box.\"  Python provides clean, intuitive and readable syntax that has become ubiquitous across in the data science industry.  Also, I find the Jupyter notebooks a convenient way to run code visually, in its attractive single document markdown format, the ability to display results of code and graphic visualizations and provide crystal-clear running documentation for future reference.   A thorough installation and importation of Python packages and libraries will provide specially designed code to perfom complex data science tasks rather than personally building them from scratch.  This will include: \n",
    "<br>&ensp; • NumPy - to work with arrays\n",
    "<br>&ensp; • Pandas - to load datasets\n",
    "<br>&ensp; • Matplotlib - to plot charts\n",
    "<br>&ensp; • Scikit-learn - for machine learning model classes\n",
    "<br>&ensp; • SciPy - for mathematical problems, specifically linear algebra transformations\n",
    "<br>&ensp; • Seaborn - for high-level interface and atttractive visualizations\n",
    "\n",
    "A quick, precise example of loading a dataset and creating a variable efficiently is using to call the Pandas library and its subsequent \"read_csv\" function in order to manipulate our data as a dataframe:\n",
    "<span style=\"color:coral\">\n",
    "<br>&ensp; import pandas as pd\n",
    "<br>&ensp; df = pd.read_csv('Data.csv')\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C4. Provide the Code</b>:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vreed\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vreed\\anaconda3\\lib\\site-packages (1.18.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\vreed\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from scipy) (1.18.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\vreed\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vreed\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vreed\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set into Pandas dataframe\n",
    "churn_df = pd.read_csv('churn_raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Churn dataframe\n",
    "churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Dataframe Columns\n",
    "df = churn_df.columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant \"Unnamed\" column at beginning & display first five records\n",
    "df = churn_df.drop(churn_df.columns[0], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename last 8 survey columns for better description of variables\n",
    "df.rename(columns = {'item1':'Responses', \n",
    "                    'item2':'Fixes', \n",
    "                     'item3':'Replacements', \n",
    "                     'item4':'Reliability', \n",
    "                     'item5':'Options', \n",
    "                     'item6':'Respectful', \n",
    "                     'item7':'Courteous', \n",
    "                     'item8':'Listening'}, \n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of records and columns of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Describe Churn dataset statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove less meaningful variables from statistics description\n",
    "df_stats = df.drop(columns=['CaseOrder', 'Zip', 'Lat', 'Lng'])\n",
    "df_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Churn Rate\n",
    "df.Churn.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review data types (numerical => \"int64\" & \"float64\"; & categorical => \"object\") in data set\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-validate column data types and missing values\n",
    "df.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display non-null fields within each columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values \n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access only rows from dataframe containing missing values\n",
    "df.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Woah, lots of empty fields!  Immediately noticeable as \"True\" in columns of \"Children\", \"Age\", \"Income\", \"Techie\", \"Phone\", \"Tenure\"\n",
    "# Display the specific columns with NAs\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm missing observations numbers\n",
    "data_nulls = df.isnull().sum()\n",
    "print(data_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store rows with missing values in a new variable\n",
    "rows_with_missing_values = df.isnull().any(axis=1)\n",
    "df[rows_with_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine columns for misspellings in categorical variables using unique() method\n",
    "df['Employment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Area'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Timezone'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well then, how many unique jobs are there and will this variable help us out much?\n",
    "len(df['Job'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Children'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine age range\n",
    "age_range = df['Age'].unique()\n",
    "print(sorted(age_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Employment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Marital'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Contract'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PaymentMethod'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display any duplicate rows in the dataframe.\n",
    "data_duplicates = df.loc[df.duplicated()]\n",
    "print(data_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the standard deviation of every numeric column in the dataset\n",
    "data_std = df_stats.std()\n",
    "print(data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nulls = df_stats.isnull().sum()\n",
    "print(data_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing fields for variables Children, Age, Income, Tenure and Bandwidth_GB_Year with median or mean\n",
    "df_stats['Children'] = df['Children'].fillna(df['Children'].median())\n",
    "df_stats['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df_stats['Income'] = df['Income'].fillna(df['Income'].median())\n",
    "df_stats['Tenure'] = df['Tenure'].fillna(df['Tenure'].median())\n",
    "df_stats['Bandwidth_GB_Year'] = df['Bandwidth_GB_Year'].fillna(df['Bandwidth_GB_Year'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nulls = df_stats.isnull().sum()\n",
    "print(data_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Anomaly Detection & Data Visualization</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms of important variables\n",
    "df_stats[['Children', 'Age', 'Income', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']].hist()\n",
    "plt.savefig('churn_pyplot.jpg')\n",
    "plt.tight_layout()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some odd distributions here, let's see some box plots for outliers\n",
    "# Create a boxplot of user duration, payment & usage variables\n",
    "df_stats.boxplot(['Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year'])\n",
    "plt.savefig('churn_boxplots.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see monthly charge separately\n",
    "df_stats.boxplot(['MonthlyCharge'])\n",
    "# Definitely outliers but not sure how that effects PCA down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a Seaborn boxplot fee & bandwidth\n",
    "sns.boxplot('MonthlyCharge', data = df_stats)\n",
    "plt.show()\n",
    "# Definitely outliers but not sure what to do with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Extract Clean dataset to 'churn_clean.csv'</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Clean dataset\n",
    "df_stats.to_csv('churn_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload cleaned data & remove all variable except user services payment info and survey data\n",
    "churn_user = pd.read_csv('churn_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Slice off all but last eleven service realted variables\n",
    "data = churn_user.loc[:, 'Tenure':'Listening']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">PCA (D206 Performance Lab)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Scikit Learn PCA application\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "churn_normalized = (data - data.mean()) / data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of components to extract\n",
    "pca = PCA(n_components = data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of PCA names\n",
    "churn_numeric = data[['Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year', 'Responses', \n",
    "                       'Fixes', 'Replacements', 'Reliability', 'Options', \n",
    "                       'Respectful', 'Courteous', 'Listening']]\n",
    "pcs_names = []\n",
    "for i, col in enumerate(churn_numeric.columns):\n",
    "    pcs_names.append('PC' + str(i + 1))\n",
    "print(pcs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Extract Clean dataset\n",
    "churn_numeric.to_csv('churn_clean_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call PCA application & convert the dataset of  19 variables into a dataset of 19 components\n",
    "pca.fit(churn_normalized)\n",
    "churn_pca = pd.DataFrame(pca.transform(churn_normalized),\n",
    "                        columns = pcs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a scree plot import matplotlib & seaborn libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scree plot\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the eigenvalues\n",
    "cov_matrix = np.dot(churn_normalized.T, churn_normalized) / data.shape[0]\n",
    "eigenvalues = [np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)) for eigenvector in pca.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigenvalues\n",
    "plt.plot(eigenvalues)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the fewest components \n",
    "for pc, var in zip(pcs_names, np.cumsum(pca.explained_variance_ratio_)):\n",
    "    print(pc, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above, we see that 89% of variance is explained by 14 components\n",
    "# Create a rotation \n",
    "rotation = pd.DataFrame(pca.components_.T, columns = pcs_names, index = churn_numeric.columns)\n",
    "print(rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output loadings for components\n",
    "loadings = pd.DataFrame(pca.components_.T,\n",
    "                       columns = pcs_names,\n",
    "                       index = data.columns)\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, extract reduced dataset & print 3 components\n",
    "churn_reduced = churn_pca.iloc[ : , 0:3]\n",
    "print(churn_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part III: Data Cleaning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">D. Data Cleaning Summary</span>\n",
    "\n",
    "D1.<b>Cleaning Findings</b>:  There was much missing data with meaningful variable fields including Children, Age, Income, Tenure and Bandwidth_GB_Year.  Given mean and variance of these variables, it seemed reasonable to impute missing values with median values.  Many categorical (such as whether or not the customer was \"Techie\") & non-numeric (columns for customer ID numbers & related customer transaction IDs) data were not included in analysis given they seemed less meaningful to interpretation and decision-making.  The anomalies discovered were not significant & were mitigated as follows.\n",
    "\n",
    "D2.<b>Justification of Mitigation Methods</b>:  Mitigated missing values with imputation using median values.  MonthlyCharge variable shows outliers so left alone. This does not seem significant to this analysis.\n",
    "\n",
    "D3.<b>Summary of Outcomes</b>:  Cleaned dataset to leave remaining variables describing customer tenure, monthly service charge, yearly bandwidth usage & responses to survey.  It seems pretty straightforward at this point.</b>\n",
    "\n",
    "D4.<b>Mitigation Code</b>:  (see code above and Panopto recording)\n",
    "\n",
    "D5.<b>Clean Data</b>: (see attached file <b style=\"color:green\">'churn_clean.csv'</b>)\n",
    "\n",
    "D6.<b>Limitations</b>: Limitations given the telecom company data set are that the data are not coming from a warehouse.  In this scenario, it is as though I initiated and gathered the data.  So, I am not able to reach out to the staff that organized & gathered this information to ask them why certain NAs are there, why are fields such as age or yearly bandwidth usage missing information that might be relevant to answering questions about customer retention or churn.  In a real world project, you would be able to go down to the department where these folks worked and fill in the empty fields or discover why fields are left blank.\n",
    "\n",
    "D7. The accurate factual data for missing fields that might be recoverable given the ability to access the staff in the data warehouse, such as company tenure with the company, may give a slighty different overall picture to the analysis. While imputation may provide a path to move forward and give decision-makers reasonable answers, there really is no reason for these data to be missing.  The limitations here could be remedied by instituting stricter data acquisition procedures, follow-ups & feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">E. PCA Application</span>\n",
    "\n",
    "E1. <b>Principal Components</b>:  The principal components, and what I determined to be \"most important\", in this dataset include survey responses to:\n",
    "<ul>\n",
    "    <li>Timely Responses</li>\n",
    "    <li>Timely Fixes</li>\n",
    "    <li>Timely Replacements</li>\n",
    "    <li>Respectful Response</li>\n",
    "</ul> \n",
    "\n",
    "E2. <b>Criteria Used</b>:  I had a sneaking intuitition that these (responses to the survey) might be the most important features to predict why a customer might or might not leave a company.  However, confirmation bias is not what confirmed this forethought.  I used a scree plot & extracted the eigenvalues for visualization of where the \"elbow was bending\".  The elbow bent at about 3 but kept an eigenvalue above 1 until the tenth component.  Then, the fewest number of components were selected based on the 89% explanation using the Numpy cumsum method. A rotation & loadings were created which suggested the \"most important\" features of the dataset.</b>\n",
    "\n",
    "E3. <b>Benefits</b>:  Their is as the loadings suggest the variables involved in timely action with regard to customer satisfaction should be given greater emphasis and hopefully help reduce the churn rate from the large number of 27% & \"increase the retention period of customers\" by targeting more resources in the direction prompt customer service (Ahmad, 2019).  Again, this seems an intuitive result but now decision-makers in the company of reasonable verification of what might have been a \"hunch\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part IV: Supporting Documents</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">F. Video</span>\n",
    "https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=fd1f0c02-19fc-487f-a797-ad1400f442cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">G. Sources for Third-Party Code</span>\n",
    "Cmdline. (2018, March 20). &ensp; <i>How To Change Column Names and Row Indexes in Pandas?</i> &ensp; Python and R Tips.  <br>&emsp;https://cmdlinetips.com/2018/03/how-to-change-column-names-and-row-indexes-in-pandas/\n",
    "\n",
    "Larose, C. D. & Larose, D. T. &ensp; (2019). &ensp; <i>Data Science: Using Python and R.</i> &ensp; John Wiley & Sons, Inc.\n",
    "\n",
    "Poulson, B. &ensp; (2016). &ensp; <i>Data Science Foundations: Data Mining</i> &ensp; LinkedIn Learning. \n",
    "<br>&emsp;https://www.linkedin.com/learning/data-science-foundations-data-mining/anomaly-detection-in-python?u=2045532\n",
    "\n",
    "Sree. &ensp; (2020, October 26). &ensp; <i>Predict Customer Churn in Python.</i> &ensp; Towards Data Science.  <br>&emsp;https://towardsdatascience.com/predict-customer-churn-in-python-e8cd6d3aaa7\n",
    "\n",
    "VanderPlas, J. &ensp; (2017). &ensp; <i>Python Data Science Handbook: Essential Tools for Working with Data.</i>  <br>&emsp;O'Reilly Media, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">H. Sources</span>\n",
    "Ahmad, A. K., Jafar, A & Aljoumaa, K. &ensp; (2019, March 20). &ensp; <i>Customer churn prediction in telecom using machine <br>&emsp;learning in big data platform</i>. &ensp; Journal of Big Data.  <br>&emsp;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0191-6\n",
    "\n",
    "Altexsoft. &ensp; (2019, March 27). &ensp; <i>Customer Churn Prediction Using Machine Learning: Main Approaches and Models</i>.  <br>&emsp;Altexsoft.  <br>&emsp;https://www.altexsoft.com/blog/business/customer-churn-prediction-for-subscription-businesses-using-machine-learning-main-approaches-and-models/\n",
    "\n",
    "Frohbose, F. &ensp; (2020, November 24). &ensp; <i>Machine Learning Case Study: Telco Customer Churn Prediction</i>.  <br>&emsp;Towards Data Science.  <br>&emsp;https://towardsdatascience.com/machine-learning-case-study-telco-customer-churn-prediction-bc4be03c9e1d\n",
    "\n",
    "Mountain, A. &ensp; (2014, August 11). &ensp; <i>Data Cleaning</i>. &ensp; Better Evaluation.  <br>&emsp;https://www.betterevaluation.org/en/evaluation-options/data_cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
