{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Assessment | D206 Data Cleaning\n",
    "&emsp;Ryan L. Buchanan\n",
    "<br>&emsp;Student ID:  001826691\n",
    "<br>&emsp;Masters Data Analytics (12/01/2020)\n",
    "<br>&emsp;Program Mentor:  Dan Estes\n",
    "<br>&emsp;(385) 432-9281 (MST)\n",
    "<br>&emsp;rbuch49@wgu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part  I: Research Question</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>A. Question or Decision</b>:</span>\n",
    "Can we determine which individual customers are at high risk of churn?  And, can we determine which features are most significant to churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>**A1. Alternative Question</b>:</span>\n",
    "Also, Are there certain responses to survey that correlate with customer churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>B. Required Variables</b>:</span>\n",
    "The data set is 10,000 customer records of a popular telecommunications company. The dependent variable (target) in question is whether or not each customer has continued or discontinued service within the last month.  This column is titled \"Churn.\"  \n",
    "Independent variables or predictors that may lead to identifying a relationship with the dependent variable of \"Churn\" within the data set include: \n",
    "1. Services that each customer signed up for (for example, multiple phone lines, technical support add-ons or streaming media) \n",
    "2. Customer account information (customers' tenure with the company, payment methods, bandwidth usage, etc.)\n",
    "3. Customer demographics (gender, marital status, income, etc.).  \n",
    "4. Finally, there are eight independent variables that represent responses customer-perceived importance of company services and features.  \n",
    "\n",
    "The data is both numerical (as in the yearly GB bandwidth usage; customer annual income) and categorical (a \"Yes\" or \"No\" for Churn; customer job)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part II: Data-Cleaning Plan</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Explanation of data cleaning plan\n",
    "1. Plan proposal to identify anomalies, <i>including relevant techniques & specific steps</i> \n",
    "2.  Justify your approach for assessing the quality of the data, include:\n",
    "<br>&ensp; •  characteristics of the data being assessed,\n",
    "<br>&ensp; •  the approach used to assess the quality.\n",
    "\n",
    "3.  Justify your selected programming language and any libraries and packages that will support the data-cleaning process.\n",
    "\n",
    "4.  Provide the code you will use to identify the anomalies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C1. Plan to Find Anomalies</b>:</span>\n",
    "My approach will include:\n",
    "<br>&ensp; 1. Backing up my data and the process I am following as a copy to my machine and, since this is a manageable data set, to GitHub using command line and gitbash;\n",
    "<br>&ensp; 2. Reading the data set into Python using Pandas read_csv command;\n",
    "<br>&ensp; 3. Naming the data set as a the variable \"churn_df\" and subsequent useful slices of the dataframe as \"data\"; \n",
    "<br>&ensp; 4. Examine coding errors, including data missing, in the collection of the data set;\n",
    "<br>&ensp; 5. Find outliers that may create or hide statistical significance using histograms;\n",
    "<br>&ensp; 6. Imputing records missing data with meaningful measures of central tendency (mean, median or mode) or simply remove outliers that are several standard deviations above the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C2. Justification of Approach</b>:</span>\n",
    "Though the data seems to be inexplicably missing quite a bit of data (such as the many NAs in customer tenure with the company) from apparently random columns, this approach seems like a good first approach in order to put the data in better working order without needing to involve methods of initial data collection or querying the data-gatherers on reasons for missing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C3. Justification of Tools</b>:</span>\n",
    "I will use the Python programming language as I have a bit of a background in Python having studied machine learning independently over the last year before beginning this masters program and its ability to perform many things right \"out of the box.\"  Python provides clean, intuitive and readable syntax that has become ubiquitous across in the data science industry.  Also, I find the Jupyter notebooks a convenient way to run code visually, in its attractive single document markdown format, the ability to display results of code and graphic visualizations and provide crystal-clear running documentation for future reference.   A thorough installation and importation of Python packages and libraries will provide specially designed code to perfom complex data science tasks rather than personally building them from scratch.  This will include: \n",
    "<br>&ensp; • NumPy - to work with arrays\n",
    "<br>&ensp; • Pandas - to load data sets\n",
    "<br>&ensp; • Matplotlib - to plot charts\n",
    "<br>&ensp; • Scikit-learn - for machine learning model classes\n",
    "<br>&ensp; • SciPy - for mathematical problems, specifically linear algebra transformations\n",
    "<br>&ensp; • Seaborn - for high-level interface and atttractive visualizations\n",
    "\n",
    "A quick, precise example of loading a data set and creating a variable efficiently is using to call the Pandas library and its subsequent \"read_csv\" function in order to manipulate our data as a dataframe:\n",
    "<span style=\"color:coral\">\n",
    "<br>&ensp; import pandas as pd\n",
    "<br>&ensp; df = pd.read_csv('Data.csv')\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"><b>C4. Provide the Code</b>:</span>\n",
    "Code follows in subsequent cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase Jupyter display cell-width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data set into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('churn_raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Churn dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just the head of the dataframe\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe Churn statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Dataframe Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(churn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of records and columns of data set\n",
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an index field\n",
    "churn_df['index'] = pd.Series(range(0,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part III: Data Cleaning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.  Summarize the data-cleaning process by doing the following:\n",
    "\n",
    "1.  Describe the findings, including all anomalies, from the implementation of the data-cleaning plan from part C.\n",
    "\n",
    "2.  Justify your methods for mitigating each type of discovered anomaly in the data set.\n",
    "\n",
    "3.  Summarize the outcome from the implementation of each data-cleaning step.\n",
    "\n",
    "4.  Provide the code used to mitigate anomalies.\n",
    "\n",
    "5.  Provide a copy of the cleaned data set.\n",
    "\n",
    "6.  Summarize the limitations of the data-cleaning process.\n",
    "\n",
    "7.  Discuss how the limitations in part D6 affect the analysis of the question or decision from part A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">D. Data Cleaning Summary</span>\n",
    "\n",
    "D1.<b>Cleaning Findings</b>:\n",
    "\n",
    "D2.<b>Justification of Mitigation Methods</b>:\n",
    "\n",
    "D3.<b>Summary of Outcomes</b>:\n",
    "\n",
    "D4.<b>Mitigation Code</b>:\n",
    "\n",
    "D5.<b>Clean Data</b>: (see attached file 'churn_data_cleaned.csv')\n",
    "\n",
    "D6.<b>Limitations</b>: Limitations given the telecom company data set are that the data are not coming from an warehouse.  It this scenario, it is as though I initiated and gathered the data.  So, I am not able to reach out to the folks that organized and gathered this information and ask them why certain NAs are there, why are fields such as age or yearly bandwidth used missing information that might be relevant to answering questions about customer retention or churn.  In a real world project, you would be able to go down to the department where these folks worked and fill in the empty fields or discover why fields are left blank.  \n",
    "D7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.  Apply principal component analysis (PCA) to identify the significant features of the data set by doing the following:\n",
    "\n",
    "1.  List the principal components in the data set.\n",
    "\n",
    "2.  Describe how you identified the principal components of the data set.\n",
    "\n",
    "3.  Describe how the organization can benefit from the results of the PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">E. PCA Application</span>\n",
    "\n",
    "E1. <b>Principal Components</b>:\n",
    "\n",
    "E2. <b>Criteria Used</b>:\n",
    "\n",
    "E3. <b>Benefits</b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Part IV: Supporting Documents</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.  Provide a Panopto recording that demonstrates the warning- and error-free functionality of the code used to support the discovery of anomalies and the data cleaning process and summarizes the programming environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">F. Video</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G.  Reference the web sources used to acquire segments of third-party code to support the application. <span style=\"color:red\">Be sure the web sources are reliable.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">G. Sources for Third-Party Code</span>\n",
    "Larose, C. D. & Larose, D. T. (2019). <i>Data Science: Using Python and R.</i>  John Wiley & Sons, Inc.\n",
    "\n",
    "VanderPlas, J. (2017). <i>Python Data Science Handbook: Essential Tools for WOrking with Data.</i>  <br>&emsp;O'Reilly Media, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H.  Acknowledge sources, <span style=\"color:red\">using in-text citations<span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">H. Sources</span>\n",
    "Ahmad, A. K., Jafar, A & Aljoumaa, K. (2019, March 20). <i>Customer churn prediction in telecom using machine <br>&emsp;learning in big data platform</i>. Journal of Big Data.  <br>&emsp;https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0191-6\n",
    "\n",
    "Altexsoft. (2019, March 27). <i>Customer Churn Prediction Using Machine Learning: Main Approaches and Models</i>.  <br>&emsp;Altexsoft.  <br>&emsp;https://www.altexsoft.com/blog/business/customer-churn-prediction-for-subscription-businesses-using-machine-learning-main-approaches-and-models/\n",
    "\n",
    "Frohbose, F. (2020, November 24). <i>Machine Learning Case Study: Telco Customer Churn Prediction</i>.  <br>&emsp;Towards Data Science.  <br>&emsp;https://towardsdatascience.com/machine-learning-case-study-telco-customer-churn-prediction-bc4be03c9e1d\n",
    "\n",
    "Mountain, A. (2014, August 11). <i>Data Cleaning</i>.  Better Evaluation.  <br>&emsp;https://www.betterevaluation.org/en/evaluation-options/data_cleaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
